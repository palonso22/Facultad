}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
colnames(lampone)
load("lampone.Rdata")
lampone$Numero <- as.numeric(lampone$Numero)
lampone$anno <- as.numeric(lampone$anno)
#lampone <- lampone[,-c(1,144)]
library(randomForest)
library(e1071)
misclass <- function(res,test){
tab <- table(res,test)
library(e1071)
optimalMatch <- matchClasses(as.matrix(tab),method="exact",verbose = TRUE)
m <- tab[,optimalMatch]
ctos <- length(test)
return((ctos-sum(diag(m)))/ctos)
}
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,]
trainClass <- class[trainIx]
test <- data[testIx,]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,,drop=FALSE]
trainClass <- class[trainIx]
test <- data[testIx,,drop=FALSE]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
load("lampone.Rdata")
#lampone$Numero <- as.numeric(lampone$Numero)
#lampone$anno <- as.numeric(lampone$anno)
lampone <- lampone[,-c(1,144)]
library(randomForest)
library(e1071)
misclass <- function(res,test){
tab <- table(res,test)
library(e1071)
optimalMatch <- matchClasses(as.matrix(tab),method="exact",verbose = TRUE)
m <- tab[,optimalMatch]
ctos <- length(test)
return((ctos-sum(diag(m)))/ctos)
}
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,,drop=FALSE]
trainClass <- class[trainIx]
test <- data[testIx,,drop=FALSE]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
load("lampone.Rdata")
#lampone$Numero <- as.numeric(lampone$Numero)
#lampone$anno <- as.numeric(lampone$anno)
lampone <- lampone[,-c(1,144)]
colnames(lampone)
load("lampone.Rdata")
load("lampone.Rdata")
colnames(lampone)
load("lampone.Rdata")
#lampone$Numero <- as.numeric(lampone$Numero)
#lampone$anno <- as.numeric(lampone$anno)
lampone <- lampone[,-c(1,144)]
library(randomForest)
library(e1071)
misclass <- function(res,test){
tab <- table(res,test)
library(e1071)
optimalMatch <- matchClasses(as.matrix(tab),method="exact",verbose = TRUE)
m <- tab[,optimalMatch]
ctos <- length(test)
return((ctos-sum(diag(m)))/ctos)
}
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,,drop=FALSE]
trainClass <- class[trainIx]
test <- data[testIx,,drop=FALSE]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
#lampone$Numero <- as.numeric(lampone$Numero)
#lampone$anno <- as.numeric(lampone$anno)
lampone <- lampone[,-c(1,144),drop=FALSE]
library(randomForest)
library(e1071)
misclass <- function(res,test){
tab <- table(res,test)
library(e1071)
optimalMatch <- matchClasses(as.matrix(tab),method="exact",verbose = TRUE)
m <- tab[,optimalMatch]
ctos <- length(test)
return((ctos-sum(diag(m)))/ctos)
}
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,,drop=FALSE]
trainClass <- class[trainIx]
test <- data[testIx,,drop=FALSE]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
dim(lampone)
load("lampone.Rdata")
dim(lampone)
#lampone$Numero <- as.numeric(lampone$Numero)
#lampone$anno <- as.numeric(lampone$anno)
lampone <- lampone[,-c(1,144),drop=FALSE]
dim(lampone)
library(randomForest)
library(e1071)
misclass <- function(res,test){
tab <- table(res,test)
library(e1071)
optimalMatch <- matchClasses(as.matrix(tab),method="exact",verbose = TRUE)
m <- tab[,optimalMatch]
ctos <- length(test)
return((ctos-sum(diag(m)))/ctos)
}
#Generic funcion to measure the performance of a machine learning algorithm
# through k-fold
testPerformance <- function(data,tuneModel,k,class,formula){
folds <- createFolds(1:nrow(data),k)
errors <- double(k)
for(i in 1:k){
print(i)
trainIx <- as.vector(unlist(folds[-i]))
testIx <- as.vector(unlist(folds[i]))
train <- data[trainIx,,drop=FALSE]
trainClass <- class[trainIx]
test <- data[testIx,,drop=FALSE]
testClass <- class[testIx]
errors[i] <- tuneModel(train,trainClass,test,testClass,formula)
cat("Error ",i,":",errors[i])
}
return(mean(errors))
}
#Random forest
rf <- function(train,trainClass,test,testClass,formula){
model <- randomForest(formula,train,ntree=1000)
return(misclass(predict(model,test),testClass))
}
#SVM polynomial kernel
polyKernel <- function(train,trainClass,test,testClass,formula){
#model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),kernel="polynomial",degree = 1:3,cost = 10^(-1:3))$best.model
model <- train(formula, data = train, method = "svmLinear")
return(misclass(predict(model,test),testClass))
}
#SVM gaussian kernel
gausKernel <- function(train,trainClass,test,testClass,formula){
model <- tune.svm(formula, data=train, tunecontrol = tune.control(sampling = "cross",cross=5),gamma = 10^(-10:0),cost = 10^(-5:5))$best.model
return(misclass(predict(model,test),testClass))
}
# Find best number of rounds for Ada boosting
tuneAda <- function(train,trainClass,test,testClass,formula){
errors <- 0
#Define a function which depends of data and rounds
eval <- function(x,xClass,y,yClass,formula,depth){
model <- boosting(formula=formula,data=x, mfinal = 50, coef="Freund", control = rpart.control(maxdepth =depth))
return(misclass(predict(model,y)$class,yClass))
}
# measure performance for ada with depth between 1:10
for (i in 1:10){
cat("depth:",i)
evalWithDepth <- function(x,xClass,y,yClass,formula) eval(x,xClass,y,yClass,formula,i)
error <- sapply(1:5,function(j) evalWithDepth(train,trainClass,test,testClass,formula))
errors[i] <- mean(error)
}
#Recover the optimal depth
optimalDepth <- which.min(errors)
cat("optimal depth",optimalDepth)
#Measure the performance with the optimal number depth
error <- eval(train,trainClass,test,testClass,formula,optimalDepth)
cat("optimal depth: ",optimalDepth)
cat("error: ",error)
return(error)
}
printRes<- function(resPoly,resGaus,resAda,resRf){
cat("SVM Poly ",resPoly)
cat("SVM Gaus ",resGaus)
cat("Ada boost ",resAda)
cat("Random Forest ",resRf)
}
colnames(lampone)
resAda <- testPerformance(lampone,tuneAda,5,lampone[,142],N_tipo~.)
sapply(1:141,function(i)is.numeric(lampone[,i]))
lampone[,142]
resAda <- testPerformance(lampone,tuneAda,5,as.numeric(lampone[,142]),N_tipo~.)
